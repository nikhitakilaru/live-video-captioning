Real-Time Live Video Captioning and Audio Narration Using Vision-Language Models 
This project presents a real-time system that captures live video from a webcam, generates descriptive captions using a pre-trained Vision-Language model (ViT-GPT2), and converts those captions into speech using a text-to-speech engine. The system integrates computer vision and natural language processing to provide immediate visual scene understanding and audio narration. It aims to enhance accessibility for visually impaired users and demonstrates the practical application of vision-language models in dynamic, real-world environments
A system designed to generate real-time captions and audio narration for live video streams.  
Utilizes vision-language models to analyze video frames and produce descriptive captions.  
Converts captions to audio output using text-to-speech technology for enhanced accessibility.  
Enables visually impaired individuals to interpret visual content effortlessly. 
Promotes inclusivity and usability across applications like education, content creation, and navigation.


